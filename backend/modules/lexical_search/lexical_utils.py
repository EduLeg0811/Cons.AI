"""
lexical_utils.py
----------------
M√≥dulo de busca l√©xica em arquivos .md/.txt e .xlsx com suporte a:

- Conectores l√≥gicos: NOT (!), AND (&), OR (|)
- Preced√™ncia: ! > & > |
- Par√™nteses
- Curingas: `*` (prefixo, sufixo, infixo)
- Frases exatas entre aspas: "campo de for√ßa"
- Normaliza√ß√£o: case-insensitive e sem acentos (NFD)
- Pr√©-filtro barato por substring (quando seguro) para acelerar a busca

Organiza√ß√£o:
1) Constantes & imports
2) Modelos de dados
3) Normaliza√ß√£o & helpers gerais
4) I/O (leitura de arquivos)
5) Mini-motor booleano (tokeniza√ß√£o, RPN, compila√ß√£o + pr√©-filtro)
6) Buscas por tipo de conte√∫do (MD/Excel)
7) Fa√ßade p√∫blica `lexical_search_in_files`
"""

from __future__ import annotations

# =============================================================================================
# 1) Constantes & imports
# =============================================================================================
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Tuple

import logging
import re
import unicodedata

import pandas as pd

from utils.config import FILES_SEARCH_DIR, MAX_OVERALL_SEARCH_RESULTS

logger = logging.getLogger("cons-ai")

# Operadores e preced√™ncia: NOT > AND > OR
_BOOL_OPS: Dict[str, int] = {"!": 3, "&": 2, "|": 1}


# =============================================================================================
# 2) Modelos de dados
# =============================================================================================
@dataclass
class SearchResult:
    """Estrutura padronizada para um item de resultado."""
    source: str
    text: str
    number: Optional[int] = None
    score: float = 0.0
    metadata: Optional[Dict[str, Any]] = None



# =============================================================================================
# 7) Public function (vers√£o atualizada)
# =============================================================================================
def lexical_search_in_files(search_term: str, source: List[str]) -> List[Dict[str, Any]]:
    """
    Busca l√©xica em m√∫ltiplos arquivos. Para cada 'book':
    - Se houver XLSX, usa esse.
    - Sen√£o, tenta MD/TXT.
    - Sen√£o, marca como ausente.

    Par√¢metros:
    - search_term: string de consulta com operadores (!, &, |), curingas (*) e frases entre aspas.
    - source: lista com nomes de "books" (sem extens√£o). Ex.: ["DAC","LO","EC"].

    Retorno:
    - Lista de dicion√°rios compat√≠vel com o restante do pipeline (source, text, number, score, metadata).
    """
    if not source:
        raise ValueError("Par√¢metro 'source' est√° vazio.")

    files_dir = Path(FILES_SEARCH_DIR)

    # -----------------------------------------------------------------------------
    # Helper: retorna Path do arquivo por prioridade XLSX > MD > TXT
    # -----------------------------------------------------------------------------
    def get_file_for_book(book: str) -> Optional[Path]:
        candidates = [
            files_dir / f"{book}.xlsx",
            files_dir / f"{book}.md",
            files_dir / f"{book}.txt",
        ]
        for c in candidates:
            if c.exists():
                return c
        return None

    # -----------------------------------------------------------------------------
    # Logging inicial
    # -----------------------------------------------------------------------------
    logger.info("\n" + "‚îÄ" * 80)
    logger.info("üìö LEXICAL SEARCH REQUEST")
    logger.info(f"üîç Termo: {search_term}")
    logger.info(f"üìò Livros solicitados: {', '.join(source)}")

    selected_files: List[Path] = []
    missing_books: List[str] = []

    for book in source:
        file_path = get_file_for_book(book)
        if file_path:
            selected_files.append(file_path)
        else:
            missing_books.append(book)

    if missing_books:
        logger.warning(f"‚ö†Ô∏è Livros sem arquivo correspondente: {', '.join(missing_books)}")

    if not selected_files:
        raise FileNotFoundError(
            f"Nenhum arquivo correspondente encontrado para os livros: {', '.join(source)}"
        )

    logger.info("CONS-AI lexical_utils.py")
    logger.info(f"[lexical_search_in_files] Arquivos selecionados: {', '.join([p.name for p in selected_files])}")

    # -----------------------------------------------------------------------------
    # Processamento dos arquivos selecionados
    # -----------------------------------------------------------------------------
    results: List[SearchResult] = []

    for path in selected_files:
        book = path.stem
        ext = path.suffix.lower()

        try:
            if ext == ".xlsx":
                rows = read_excel_first_sheet(path)
                matches = search_excel_rows(rows, search_term)
                for m in matches:
                    results.append(SearchResult(
                        source=book,
                        text=m.get("paragraph_text", ""),
                        number=m.get("paragraph_number"),
                        score=0.0,
                        metadata=m.get("metadata")
                    ))

            elif ext in {".md", ".txt"}:
                text = read_text_file(path)
                matches = search_md_content(text, search_term)
                for m in matches:
                    results.append(SearchResult(
                        source=book,
                        text=m.get("paragraph_text", ""),
                        number=m.get("paragraph_number"),
                        score=0.0,
                        metadata=None
                    ))

        except Exception as e:
            logger.error(f"[lexical_search_in_files] Erro ao processar {path.name}: {e}", exc_info=True)

    # -----------------------------------------------------------------------------
    # Limita resultados globais e devolve no formato esperado (dict)
    # -----------------------------------------------------------------------------
    results = clamp_max_results(results, MAX_OVERALL_SEARCH_RESULTS)

    logger.info(f"[lexical_search_in_files] Total de resultados: {len(results)}")

    return [asdict(r) for r in results]
















# =============================================================================================
# 3) Normaliza√ß√£o & helpers gerais
# =============================================================================================
def strip_accents(s: str) -> str:
    """Remove acentos mantendo apenas as letras base (NFD)."""
    if not s:
        return ""
    return "".join(
        c for c in unicodedata.normalize("NFD", s) if unicodedata.category(c) != "Mn"
    )


def normalize_for_match(s: str) -> str:
    """Normaliza string para compara√ß√£o: sem acentos e min√∫scula."""
    # Remove acentos, converte para min√∫sculas e elimina pontua√ß√µes em geral,
    # preservando apenas caracteres de palavra, espa√ßo e '*' (curinga).
    base = strip_accents(s or "").lower()
    # Remove tudo que N√ÉO √© \w (letra/d√≠gito/_), espa√ßo ou '*':
    return re.sub(r"[^\w\s\*]", "", base)


def balanced_parentheses(query: str) -> bool:
    """Retorna True se os par√™nteses estiverem balanceados."""
    depth = 0
    for ch in query:
        if ch == "(":
            depth += 1
        elif ch == ")":
            depth -= 1
            if depth < 0:
                return False
    return depth == 0


def clamp_max_results(results: List[Any], limit: int) -> List[Any]:
    """Corta a lista ao tamanho m√°ximo (sem causar exce√ß√µes se limite <= 0)."""
    return results[: max(0, limit)]


def strip_markdown_simple(s: str) -> str:
    """
    Remove marca√ß√µes markdown simples (* e **) usadas para it√°lico/negrito.
    Ex.: "**texto**" -> "texto"; "*texto*" -> "texto".
    """
    if not s:
        return ""
    # remove ** e *
    return re.sub(r"(\*\*|\*)", "", s)



# =============================================================================================
# 4) I/O (leitura de arquivos)
# =============================================================================================
def list_files(source_dir: str, extension: str) -> List[Path]:
    """
    Lista arquivos de uma extens√£o num diret√≥rio (n√£o recursivo).
    - extension: "md", "txt" ou "xlsx"
    """
    base = Path(source_dir).expanduser().resolve()
    if not base.exists():
        logging.error(f"Diret√≥rio n√£o existe: {base}")
        return []
    ext = f".{str(extension).lower()}"
    return sorted([p for p in base.iterdir() if p.is_file() and p.suffix.lower() == ext])


def read_text_file(path: Path, encodings: Tuple[str, ...] = ("utf-8", "cp1252")) -> str:
    """
    L√™ um arquivo texto/markdown testando m√∫ltiplos encodings.
    Levanta exce√ß√£o se todos falharem.
    """
    last_error: Optional[Exception] = None
    for enc in encodings:
        try:
            return path.read_text(encoding=enc)
        except UnicodeDecodeError as e:
            last_error = e
            logging.warning(f"[read_text_file] Falha {path.name} com {enc}. Tentando pr√≥ximo‚Ä¶")
    raise Exception(f"N√£o foi poss√≠vel decodificar {path} ({encodings}). √öltimo erro: {last_error}")


def read_excel_first_sheet(path: Path) -> List[Dict[str, str]]:
    """
    L√™ a primeira planilha como lista de dicion√°rios (tudo como string).
    Adiciona 'paragraph_number' como n√∫mero de linha (1-based no dado).
    """
    df = pd.read_excel(path, sheet_name=0, dtype=str).fillna("")
    rows: List[Dict[str, str]] = []
    for i, row in enumerate(df.to_dict(orient="records"), start=1):
        # normaliza chaves em min√∫sculas para evitar colis√µes/exce√ß√µes posteriores
        row_norm = {str(k).lower(): ("" if v is None else str(v)) for k, v in row.items()}
        row_norm["paragraph_number"] = i
        rows.append(row_norm)
    return rows


# =============================================================================================
# 5) Mini-motor booleano (tokeniza√ß√£o, RPN, compila√ß√£o + pr√©-filtro)
# =============================================================================================
def tokenize_query(q: str) -> List[str]:
    """
    Tokeniza conectores, par√™nteses e termos.
    - Suporta frases entre aspas duplas como um √∫nico token (pode conter espa√ßos).
    - Ex.: pato & "donald duck" | !cadeira -> ['pato','&','\"donald duck\"','|','!','cadeira']
    """
    tokens: List[str] = []
    i, n = 0, len(q)
    while i < n:
        c = q[i]
        if c.isspace():
            i += 1
            continue
        if c in "()&|!":
            tokens.append(c)
            i += 1
            continue
        if c == '"':
            # frase entre aspas
            j = i + 1
            buf: List[str] = []
            while j < n and q[j] != '"':
                buf.append(q[j])
                j += 1
            tokens.append('"' + "".join(buf) + '"')
            i = j + 1 if j < n and q[j] == '"' else j
            continue
        # termo simples at√© operador/espa√ßo/aspas
        j = i
        while j < n and (q[j] not in '()&|!"') and (not q[j].isspace()):
            j += 1
        tokens.append(q[i:j])
        i = j
    # remove tokens vazios (p. ex., se houver m√∫ltiplos espa√ßos)
    return [t for t in tokens if t]


def shunting_yard(tokens: List[str]) -> List[str]:
    """Converte express√£o infixa -> p√≥s-fixa (RPN), respeitando preced√™ncia."""
    out: List[str] = []
    st: List[str] = []
    for t in tokens:
        if t in _BOOL_OPS:
            while st and st[-1] in _BOOL_OPS and _BOOL_OPS[st[-1]] >= _BOOL_OPS[t]:
                out.append(st.pop())
            st.append(t)
        elif t == "(":
            st.append(t)
        elif t == ")":
            while st and st[-1] != "(":
                out.append(st.pop())
            if not st:
                logging.warning("[shunting_yard] Par√™ntese fechado sem correspondente.")
                continue
            st.pop()  # remove '('
        else:
            out.append(t)  # termo/frase
    while st:
        top = st.pop()
        if top in "()":
            logging.warning("[shunting_yard] Par√™nteses n√£o balanceados ao final.")
            continue
        out.append(top)
    return out


def wildcard_pattern(term_raw: str) -> re.Pattern:
    """
    Converte termo com * em regex (sobre texto normalizado).
    - Usa limites de palavra apenas se N√ÉO houver * no in√≠cio/fim.
    """
    # normalizamos o termo como normalizamos o texto
    term = normalize_for_match(term_raw)
    escaped = re.escape(term).replace(r"\*", ".*")

    prefix_bound = not term_raw.startswith("*")
    suffix_bound = not term_raw.endswith("*")

    pattern_str = ""
    if prefix_bound:
        pattern_str += r"\b"
    pattern_str += escaped
    if suffix_bound:
        pattern_str += r"\b"

    return re.compile(pattern_str, flags=re.IGNORECASE)


def phrase_pattern(quoted_raw: str) -> re.Pattern:
    """
    Frase entre aspas: busca de substring literal (sem curingas), insens√≠vel a caso/acentos.
    """
    core = quoted_raw[1:-1] if quoted_raw.startswith('"') and quoted_raw.endswith('"') else quoted_raw
    core_norm = normalize_for_match(core)
    return re.compile(re.escape(core_norm), flags=re.IGNORECASE)


def compile_boolean_predicate(query: str) -> Callable[[str], bool]:
    """
    Compila a query textual em um predicado (pnorm: str) -> bool, onde `pnorm`
    √© o par√°grafo previamente normalizado (normalize_for_match).

    Regras:
      - "frase exata" -> substring literal normalizada
      - termo com *   -> wildcard (.*)
      - termo sem *   -> palavra inteira (\b...\b)
      - conectores    -> !, &, |   (preced√™ncia ! > & > |)
      - par√™nteses    -> opcionais
    """
    q = (query or "").strip()
    if q and ('"' not in q) and ('*' not in q) and all(op not in q for op in ('&', '|', '!', '(', ')')) and any(ch.isspace() for ch in q):
        q = '"' + q + '"'
    if not q:
        # query vazia nunca casa nada
        return lambda _: False

    # valida√ß√£o leve (opcional, s√≥ loga)
    if not balanced_parentheses(q):
        logging.warning("[compile_boolean_predicate] Par√™nteses possivelmente desbalanceados.")

    tokens = tokenize_query(q)
    rpn = shunting_yard(tokens)

    # cache de padr√µes por token; evita recompilar o mesmo regex
    pat_cache: Dict[str, re.Pattern] = {}

    def make_term_pred(token: str) -> Callable[[str], bool]:
        """Constr√≥i predicado para 'token' (termo simples ou frase entre aspas)."""
        # frase entre aspas?
        if len(token) >= 2 and token[0] == '"' and token[-1] == '"':
            if token not in pat_cache:
                pat_cache[token] = phrase_pattern(token)
            pat = pat_cache[token]
            return lambda s: bool(pat.search(s))

        # termo comum: com/sem '*'
        if token not in pat_cache:
            if "*" in token:
                pat_cache[token] = wildcard_pattern(token)
            else:
                norm = normalize_for_match(token)
                # Default: prefix match at word start (behaves like implicit trailing '*')
                # Example: 'casa' matches 'casa', 'casado', but not 'emcasacado'.
                pat_cache[token] = re.compile(rf"\b{re.escape(norm)}\b", flags=re.IGNORECASE)
        pat = pat_cache[token]
        return lambda s: bool(pat.search(s))

    # monta a fun√ß√£o via RPN
    stack: List[Callable[[str], bool]] = []
    for t in rpn:
        if t in _BOOL_OPS:
            try:
                if t == "!":
                    a = stack.pop()
                    stack.append(lambda s, a=a: not a(s))
                elif t == "&":
                    b, a = stack.pop(), stack.pop()
                    stack.append(lambda s, a=a, b=b: a(s) and b(s))
                elif t == "|":
                    b, a = stack.pop(), stack.pop()
                    stack.append(lambda s, a=a, b=b: a(s) or b(s))
            except IndexError:
                logging.error("[compile_boolean_predicate] Express√£o booleana inv√°lida (operandos insuficientes).")
                # retorna predicado que falha sempre para n√£o quebrar o fluxo
                return lambda _: False
        else:
            stack.append(make_term_pred(t))

    if not stack:
        # Query s√≥ com operadores ou vazia
        return lambda _: False

    return stack[-1]


# ----------------------------- PR√â-FILTRO BARATO POR SUBSTRING --------------------------------
def _has_or(tokens: List[str]) -> bool:
    """Retorna True se a express√£o possui operador OR (|)."""
    return "|" in tokens


def _extract_conj_literals_for_prefilter(tokens: List[str]) -> Tuple[List[str], List[str]]:
    """
    Extrai literais para pr√©-filtro quando a express√£o √© CONJUN√á√ÉO pura (sem OR).
    Retorna (must_have, must_not), j√° normalizados (sem acentos, min√∫sculos).

    Regras:
      - Inclui frases entre aspas e termos simples SEM '*'.
      - Ignora termos com curinga.
      - Considera nega√ß√£o un√°ria '!' no token imediatamente seguinte.
    """
    must_have: List[str] = []
    must_not: List[str] = []

    negate_next = False
    for t in tokens:
        if t == "!":
            negate_next = True
            continue
        if t in ("&", "(", ")"):
            continue
        if t == "|":
            # seguran√ßa extra ‚Äî n√£o usar prefilter se houver OR
            return [], []

        # t √© termo/frase; ignorar se tem curinga
        is_phrase = (len(t) >= 2 and t[0] == '"' and t[-1] == '"')
        core = t[1:-1] if is_phrase else t

        if "*" in core:
            negate_next = False
            continue

        lit = normalize_for_match(core)

        if negate_next:
            must_not.append(lit)
        else:
            must_have.append(lit)
        negate_next = False

    return must_have, must_not


def compile_prefilter(query: str) -> Optional[Callable[[str], bool]]:
    """
    Compila um pr√©-filtro barato (checks de substring) OU retorna None se n√£o for seguro aplicar.
    S√≥ ativa para CONJUN√á√ÉO pura (sem OR) e sem curingas nos literais pr√©-filtrados.
    """
    q = (query or "").strip()
    if q and ('"' not in q) and ('*' not in q) and all(op not in q for op in ('&', '|', '!', '(', ')')) and any(ch.isspace() for ch in q):
        q = '"' + q + '"'
    if not q:
        return None

    tokens = tokenize_query(q)
    if _has_or(tokens):
        return None  # desliga quando h√° OR

    must_have, must_not = _extract_conj_literals_for_prefilter(tokens)
    if not must_have and not must_not:
        return None

    def _prefilter(pnorm: str) -> bool:
        # todos obrigat√≥rios presentes
        for lit in must_have:
            if lit not in pnorm:
                return False
        # nenhum proibido presente
        for lit in must_not:
            if lit in pnorm:
                return False
        return True

    return _prefilter


# =============================================================================================
# 6) Buscas por tipo de conte√∫do (MD/Excel)
# =============================================================================================
def process_found_paragraph(paragraph: str, search_term: str) -> str:
    """
    Reestrutura par√°grafos que usam '|' como agregador:
      - Se houver 2+ ocorr√™ncias de '|': mant√©m o primeiro "cabe√ßalho" e
        adiciona apenas subtrechos que contenham o termo de busca (normalizado).
      - Caso contr√°rio, retorna o par√°grafo original.

    Obs.: mant√©m acentua√ß√£o/caixa do texto original (apenas a checagem √© normalizada).
    """
    if not search_term:
        return paragraph

    needle = normalize_for_match(search_term)

    if paragraph.count("|") >= 2:
        parts = paragraph.split("|")
        if not parts:
            return paragraph

        rebuilt: List[str] = [parts[0].strip()]
        for sub in parts[1:]:
            s = sub.strip()
            if needle in normalize_for_match(s):
                rebuilt.append(s)

        # se nenhum subtrecho relevante foi encontrado, descarta o par√°grafo
        if len(rebuilt) == 1:
            return ""

        result = " ".join(rebuilt)
        return result.replace("|", "").replace("\\", "").replace("\n", "").strip()

    return paragraph


def search_md_content(content: str, query: str) -> List[Dict[str, Any]]:
    """
    Aplica a busca booleana em conte√∫do de texto/markdown.
    Retorna dicion√°rios simples para posterior montagem de SearchResult.
    """
    if not content or not query:
        return []

    # 1 par√°grafo = 1 linha n√£o vazia
    paragraphs: List[str] = [p.strip() for p in content.split("\n") if p.strip()]

    pred = compile_boolean_predicate(query)
    pre = compile_prefilter(query)  # pr√©-filtro barato (pode ser None)

    results: List[Dict[str, Any]] = []

    for idx, paragraph in enumerate(paragraphs, start=1):
        # remove marca√ß√µes markdown antes de normalizar
        cleaned = strip_markdown_simple(paragraph)
        pnorm = normalize_for_match(cleaned)


        # 1) pr√©-filtro barato (quando aplic√°vel)
        if pre is not None and not pre(pnorm):
            continue

        # 2) predicado completo (regex/curingas/aspas/conectores)
        if pred(pnorm):
            processed = process_found_paragraph(paragraph, query)
            if processed and processed.strip():
                results.append({"paragraph_text": processed, "paragraph_number": idx})
        if len(results) >= MAX_OVERALL_SEARCH_RESULTS:
            break

    return results


def search_excel_rows(rows: List[Dict[str, Any]], query: str) -> List[Dict[str, Any]]:
    """
    Aplica a busca booleana em linhas de Excel (primeira coluna textual √© a "principal").
    Retorna dicion√°rios simples para posterior montagem de SearchResult.
    """
    if not rows or not query:
        return []

    # normaliza chaves (defensivo ‚Äì j√° normalizamos em read_excel_first_sheet)
    rows = [{k.lower(): v for k, v in row.items()} for row in rows]

    # primeira coluna de dados (ordem preservada pelo pandas; se vazio, aborta)
    first_row = rows[0]
    if not first_row:
        return []

    texto_key = list(first_row.keys())[0]  # "primeira coluna"
    pred = compile_boolean_predicate(query)
    pre = compile_prefilter(query)  # pr√©-filtro barato (pode ser None)

    results: List[Dict[str, Any]] = []

    for row in rows:
        paragraph = str(row.get(texto_key, ""))
        
        cleaned = strip_markdown_simple(paragraph)
        pnorm = normalize_for_match(cleaned)


        # 1) pr√©-filtro barato (quando aplic√°vel)
        if pre is not None and not pre(pnorm):
            continue

        # 2) predicado completo
        if pred(pnorm):
            processed = process_found_paragraph(paragraph, query)
            if processed and processed.strip():
                number = row.get("paragraph_number")
                results.append({
                    "paragraph_text": processed,
                    "paragraph_number": int(number) if str(number).isdigit() else None,
                    "metadata": row
                })
        if len(results) >= MAX_OVERALL_SEARCH_RESULTS:
            break

    return results


# =============================================================================================
# Notas de manuten√ß√£o
# ---------------------------------------------------------------------------------------------
# - O pr√©-filtro barato acelera muito cole√ß√µes grandes sem comprometer a corre√ß√£o,
#   pois s√≥ ativa em CONJUN√á√ÉO pura (sem OR) e sem curingas nos literais.
# - Para aspas simples como frase exata, duplique a l√≥gica de phrase_pattern para "'‚Ä¶'".
# - Para destacar trechos no `text` (HTML), devolva offsets do regex ao inv√©s de apenas True/False.
# - Se quiser pr√©-filtros mais sofisticados (com OR), √© poss√≠vel analisar a √°rvore booleana e
#   construir conjuntos "pelo menos um" para cada cl√°usula ‚Äî fica mais complexo, mas vi√°vel.
# =============================================================================================
